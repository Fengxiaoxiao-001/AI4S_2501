import math
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import json
import os
import glob
from collections import defaultdict


def parse_question_id(question_id):
    """è§£æé¢˜ç›®IDï¼Œæå–éš¾åº¦ã€ä¸»é¢˜ã€é¢˜å‹ä¿¡æ¯"""
    # æ ¹æ®ä½ çš„å®é™…å‘½åè§„åˆ™è°ƒæ•´
    if question_id.startswith('Easy'):
        difficulty = 'Easy'
        rest = question_id[4:]
    elif question_id.startswith('Medium'):
        difficulty = 'Medium'
        rest = question_id[6:]
    elif question_id.startswith('Hard'):
        difficulty = 'Hard'
        rest = question_id[4:]
    else:
        difficulty = 'Unknown'
        rest = question_id

    # è§£æé¢˜å‹å’Œä¸»é¢˜
    question_type = rest[:2]  # ç¤ºä¾‹: Aq
    topic = rest[2:]  # ç¤ºä¾‹: Geom

    return {
        'difficulty': difficulty,
        'question_type': question_type,
        'topic': topic
    }


def load_all_json_data(base_path):
    """
    åŠ è½½æ‰€æœ‰JSONæ•°æ®ï¼Œè€ƒè™‘ä¸åŒçš„æç¤ºè¯å·¥ç¨‹ç­–ç•¥
    base_path: æ ¹ç›®å½•è·¯å¾„ï¼Œå¦‚ "E:/Preprocessing/AI4S_2/Math/"
    """
    all_data = {}

    # å®šä¹‰æç¤ºè¯å·¥ç¨‹ç±»å‹
    prompt_strategies = {
        "æ— å¤„ç†": "no_processing",
        "å¤šæ™ºèƒ½ä½“": "multi_agent",
        "è§’è‰²æ‰®æ¼”": "role_playing"
    }

    # å®šä¹‰éš¾åº¦çº§åˆ«
    difficulties = ["ç®€å•", "ä¸­ç­‰", "å›°éš¾"]

    # éå†æ‰€æœ‰ç›®å½•ç»“æ„
    for difficulty in difficulties:
        for strategy_name, strategy_code in prompt_strategies.items():
            # æ„å»ºæœç´¢æ¨¡å¼
            search_pattern = os.path.join(
                base_path,
                difficulty,
                strategy_name,
                "*",  # é¢˜å‹
                "*",  # ä¸»é¢˜
                "*",  # é¢˜ç›®ç¼–å·
                "answer.json"
            )

            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„JSONæ–‡ä»¶
            json_files = glob.glob(search_pattern)

            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)

                    # æå–é¢˜ç›®IDå’Œé¢˜ç›®æ•°æ®
                    for question_id, question_data in file_data.items():
                        # æ·»åŠ æç¤ºè¯å·¥ç¨‹ä¿¡æ¯
                        question_data['prompt_strategy'] = strategy_name
                        question_data['prompt_strategy_code'] = strategy_code
                        question_data['file_path'] = json_file

                        # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
                        all_data[question_id] = question_data

                except Exception as e:
                    print(f"Error loading {json_file}: {e}")

    print(f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªé¢˜ç›®æ•°æ®")
    return all_data


def comprehensive_efficiency_analysis_all_questions(full_json_data):
    """åˆ†ææ‰€æœ‰é¢˜ç›®çš„æ•ˆç‡ï¼ˆä¿ç•™åŒæ•ˆç‡æŒ‡æ ‡ï¼‰"""

    all_results = []

    for question_id, question_data in full_json_data.items():
        # è§£æé¢˜ç›®ä¿¡æ¯
        question_info = parse_question_id(question_id)

        # æ·»åŠ æç¤ºè¯å·¥ç¨‹ä¿¡æ¯
        question_info['prompt_strategy'] = question_data.get('prompt_strategy', 'Unknown')
        question_info['prompt_strategy_code'] = question_data.get('prompt_strategy_code', 'unknown')

        if 'models' in question_data:
            for model_name, model_data in question_data['models'].items():
                if model_data['success']:
                    rt = model_data['response_time']
                    content = model_data['content']
                    content_len = len(content)

                    # ä¿ç•™åŒæ•ˆç‡æŒ‡æ ‡
                    raw_efficiency = content_len / rt
                    normalized_efficiency = math.log(content_len + 1) / rt

                    result = {
                        'question_id': question_id,
                        'model': model_name,
                        'response_time': rt,
                        'content_length': content_len,
                        'raw_efficiency': raw_efficiency,
                        'normalized_efficiency': normalized_efficiency,
                        'efficiency_ratio': normalized_efficiency / raw_efficiency if raw_efficiency > 0 else 0,
                        'prompt_strategy': question_info['prompt_strategy'],
                        'prompt_strategy_code': question_info['prompt_strategy_code']
                    }

                    # æ·»åŠ é¢˜ç›®åˆ†ç±»ä¿¡æ¯
                    result.update(question_info)
                    all_results.append(result)

    df = pd.DataFrame(all_results)
    return df


def create_comprehensive_analysis_charts(df):
    """åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨ - ç»“åˆåŒæ•ˆç‡å¯¹æ¯”ã€åˆ†å±‚åˆ†æå’Œæç¤ºè¯å·¥ç¨‹åˆ†æ"""

    # åˆ›å»º3x3çš„å­å›¾å¸ƒå±€
    fig, axes = plt.subplots(3, 3, figsize=(24, 18))

    # 1. æ€»ä½“åŒæ•ˆç‡å¯¹æ¯”ï¼ˆå·¦ä¸Šï¼‰
    model_avg = df.groupby('model').agg({
        'raw_efficiency': 'mean',
        'normalized_efficiency': 'mean',
        'efficiency_ratio': 'mean'
    }).reset_index()

    # åŸå§‹æ•ˆç‡æ’å
    model_avg_raw = model_avg.sort_values('raw_efficiency', ascending=False)
    axes[0, 0].bar(model_avg_raw['model'], model_avg_raw['raw_efficiency'],
                   color='lightblue', alpha=0.7)
    axes[0, 0].set_title('ğŸ“Š æ€»ä½“åŸå§‹æ•ˆç‡æ’å\n(å­—ç¬¦/ç§’)', fontsize=12, fontweight='bold')
    axes[0, 0].tick_params(axis='x', rotation=45)

    # 2. æ ‡å‡†åŒ–æ•ˆç‡æ’åï¼ˆä¸­ä¸Šï¼‰
    model_avg_norm = model_avg.sort_values('normalized_efficiency', ascending=False)
    axes[0, 1].bar(model_avg_norm['model'], model_avg_norm['normalized_efficiency'],
                   color='lightgreen', alpha=0.7)
    axes[0, 1].set_title('âš¡ æ€»ä½“æ ‡å‡†åŒ–æ•ˆç‡æ’å\n(log(é•¿åº¦)/ç§’)', fontsize=12, fontweight='bold')
    axes[0, 1].tick_params(axis='x', rotation=45)

    # 3. æ•ˆç‡æ¯”å€¼åˆ†æï¼ˆå³ä¸Šï¼‰
    model_avg_ratio = model_avg.sort_values('efficiency_ratio', ascending=False)
    axes[0, 2].bar(model_avg_ratio['model'], model_avg_ratio['efficiency_ratio'],
                   color='orange', alpha=0.7)
    axes[0, 2].set_title('ğŸ’ æ•ˆç‡æ¯”å€¼æ’å\n(æ ‡å‡†åŒ–/åŸå§‹)', fontsize=12, fontweight='bold')
    axes[0, 2].tick_params(axis='x', rotation=45)
    axes[0, 2].axhline(y=1, color='red', linestyle='--', alpha=0.5)

    # 4. æŒ‰éš¾åº¦åˆ†å±‚çš„åŸå§‹æ•ˆç‡ï¼ˆä¸­å·¦ï¼‰
    difficulties = ['Easy', 'Medium', 'Hard']
    colors = ['#ff9999', '#66b3ff', '#99ff99']

    # å‡†å¤‡æ•°æ®
    difficulty_data = []
    for difficulty in difficulties:
        df_diff = df[df['difficulty'] == difficulty]
        diff_avg = df_diff.groupby('model')['raw_efficiency'].mean().reset_index()
        diff_avg['difficulty'] = difficulty
        difficulty_data.append(diff_avg)

    # åˆ›å»ºåˆ†ç»„æŸ±çŠ¶å›¾
    difficulty_df = pd.concat(difficulty_data)
    pivot_df = difficulty_df.pivot(index='model', columns='difficulty', values='raw_efficiency')

    x = np.arange(len(pivot_df.index))
    width = 0.25

    for i, difficulty in enumerate(difficulties):
        axes[1, 0].bar(x + i * width, pivot_df[difficulty], width,
                       label=difficulty, color=colors[i], alpha=0.7)

    axes[1, 0].set_xlabel('æ¨¡å‹')
    axes[1, 0].set_ylabel('åŸå§‹æ•ˆç‡')
    axes[1, 0].set_title('ğŸ“ˆ æŒ‰éš¾åº¦åˆ†å±‚çš„åŸå§‹æ•ˆç‡', fontsize=12, fontweight='bold')
    axes[1, 0].set_xticks(x + width)
    axes[1, 0].set_xticklabels(pivot_df.index, rotation=45)
    axes[1, 0].legend()

    # 5. å“åº”æ—¶é—´çƒ­åŠ›å›¾ï¼ˆä¸­ä¸­ï¼‰- æŒ‰éš¾åº¦å’Œæ¨¡å‹
    time_pivot = df.pivot_table(values='response_time',
                                index='model',
                                columns='difficulty',
                                aggfunc='mean')

    im = axes[1, 1].imshow(time_pivot, cmap='YlOrRd', aspect='auto')
    axes[1, 1].set_title('â±ï¸ å¹³å‡å“åº”æ—¶é—´çƒ­åŠ›å›¾', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(range(len(time_pivot.columns)))
    axes[1, 1].set_xticklabels(time_pivot.columns)
    axes[1, 1].set_yticks(range(len(time_pivot.index)))
    axes[1, 1].set_yticklabels(time_pivot.index)
    plt.colorbar(im, ax=axes[1, 1])

    # 6. æ•ˆç‡-æ—¶é—´æ•£ç‚¹å›¾ï¼ˆä¸­å³ï¼‰
    avg_data = df.groupby('model').agg({
        'raw_efficiency': 'mean',
        'response_time': 'mean',
        'normalized_efficiency': 'mean'
    }).reset_index()

    scatter = axes[1, 2].scatter(avg_data['response_time'],
                                 avg_data['raw_efficiency'],
                                 s=avg_data['normalized_efficiency'] * 100,
                                 alpha=0.7, cmap='viridis')

    for i, row in avg_data.iterrows():
        axes[1, 2].annotate(row['model'],
                            (row['response_time'], row['raw_efficiency']),
                            xytext=(5, 5), textcoords='offset points',
                            fontsize=8)

    axes[1, 2].set_xlabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')
    axes[1, 2].set_ylabel('å¹³å‡åŸå§‹æ•ˆç‡')
    axes[1, 2].set_title('ğŸ”„ æ•ˆç‡-æ—¶é—´å…³ç³»å›¾\n(ç‚¹å¤§å°åæ˜ æ ‡å‡†åŒ–æ•ˆç‡)', fontsize=12, fontweight='bold')
    axes[1, 2].grid(True, alpha=0.3)

    # 7. æŒ‰æç¤ºè¯å·¥ç¨‹ç­–ç•¥åˆ†æï¼ˆå·¦ä¸‹ï¼‰- åŸå§‹æ•ˆç‡
    prompt_strategies = df['prompt_strategy'].unique()
    prompt_data = []

    for strategy in prompt_strategies:
        df_strategy = df[df['prompt_strategy'] == strategy]
        strategy_avg = df_strategy.groupby('model')['raw_efficiency'].mean().reset_index()
        strategy_avg['strategy'] = strategy
        prompt_data.append(strategy_avg)

    prompt_df = pd.concat(prompt_data)
    prompt_pivot = prompt_df.pivot(index='model', columns='strategy', values='raw_efficiency')

    x_prompt = np.arange(len(prompt_pivot.index))
    width_prompt = 0.25

    for i, strategy in enumerate(prompt_strategies):
        if strategy in prompt_pivot.columns:
            axes[2, 0].bar(x_prompt + i * width_prompt, prompt_pivot[strategy], width_prompt,
                           label=strategy, alpha=0.7)

    axes[2, 0].set_xlabel('æ¨¡å‹')
    axes[2, 0].set_ylabel('åŸå§‹æ•ˆç‡')
    axes[2, 0].set_title('ğŸ­ æŒ‰æç¤ºè¯å·¥ç¨‹ç­–ç•¥çš„æ•ˆç‡å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[2, 0].set_xticks(x_prompt + width_prompt)
    axes[2, 0].set_xticklabels(prompt_pivot.index, rotation=45)
    axes[2, 0].legend()

    # 8. æç¤ºè¯å·¥ç¨‹ç­–ç•¥æ•ˆæœçƒ­åŠ›å›¾ï¼ˆä¸‹ä¸­ï¼‰
    strategy_time_pivot = df.pivot_table(values='response_time',
                                         index='model',
                                         columns='prompt_strategy',
                                         aggfunc='mean')

    im_strategy = axes[2, 1].imshow(strategy_time_pivot, cmap='YlOrRd', aspect='auto')
    axes[2, 1].set_title('ğŸ”§ æç¤ºè¯å·¥ç¨‹ç­–ç•¥å“åº”æ—¶é—´çƒ­åŠ›å›¾', fontsize=12, fontweight='bold')
    axes[2, 1].set_xticks(range(len(strategy_time_pivot.columns)))
    axes[2, 1].set_xticklabels(strategy_time_pivot.columns)
    axes[2, 1].set_yticks(range(len(strategy_time_pivot.index)))
    axes[2, 1].set_yticklabels(strategy_time_pivot.index)
    plt.colorbar(im_strategy, ax=axes[2, 1])

    # 9. æç¤ºè¯å·¥ç¨‹ç­–ç•¥æ•ˆæœå¯¹æ¯”ï¼ˆå³ä¸‹ï¼‰
    strategy_effectiveness = df.groupby(['model', 'prompt_strategy']).agg({
        'raw_efficiency': 'mean',
        'response_time': 'mean'
    }).reset_index()

    # è®¡ç®—æ¯ä¸ªæ¨¡å‹åœ¨ä¸åŒç­–ç•¥ä¸‹çš„æ•ˆç‡æå‡
    model_strategy_comparison = []
    for model in df['model'].unique():
        model_data = strategy_effectiveness[strategy_effectiveness['model'] == model]
        if len(model_data) > 1:
            # æ‰¾åˆ°æœ€ä½³ç­–ç•¥
            best_strategy = model_data.loc[model_data['raw_efficiency'].idxmax()]
            worst_strategy = model_data.loc[model_data['raw_efficiency'].idxmin()]
            improvement = ((best_strategy['raw_efficiency'] - worst_strategy['raw_efficiency']) /
                           worst_strategy['raw_efficiency']) * 100

            model_strategy_comparison.append({
                'model': model,
                'best_strategy': best_strategy['prompt_strategy'],
                'best_efficiency': best_strategy['raw_efficiency'],
                'worst_strategy': worst_strategy['prompt_strategy'],
                'worst_efficiency': worst_strategy['raw_efficiency'],
                'improvement_percent': improvement
            })

    comparison_df = pd.DataFrame(model_strategy_comparison)
    if not comparison_df.empty:
        comparison_df = comparison_df.sort_values('improvement_percent', ascending=False)
        bars = axes[2, 2].bar(comparison_df['model'], comparison_df['improvement_percent'],
                              color='purple', alpha=0.7)
        axes[2, 2].set_xlabel('æ¨¡å‹')
        axes[2, 2].set_ylabel('æ•ˆç‡æå‡ç™¾åˆ†æ¯” (%)')
        axes[2, 2].set_title('ğŸ“Š æç¤ºè¯å·¥ç¨‹ç­–ç•¥æ•ˆæœæå‡å¯¹æ¯”', fontsize=12, fontweight='bold')
        axes[2, 2].tick_params(axis='x', rotation=45)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, improvement in zip(bars, comparison_df['improvement_percent']):
            axes[2, 2].text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
                            f'{improvement:.1f}%', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    return fig


def print_detailed_efficiency_insights(df):
    """æ‰“å°è¯¦ç»†çš„æ•ˆç‡åˆ†ææ´å¯Ÿï¼ŒåŒ…æ‹¬æç¤ºè¯å·¥ç¨‹æ•ˆæœ"""

    print("=" * 80)
    print("ğŸ“Š è¯¦ç»†æ•ˆç‡åˆ†ææŠ¥å‘Šï¼ˆåŒ…å«æç¤ºè¯å·¥ç¨‹æ•ˆæœï¼‰")
    print("=" * 80)

    # æ€»ä½“ç»Ÿè®¡
    overall_stats = df.groupby('model').agg({
        'raw_efficiency': ['mean', 'std'],
        'normalized_efficiency': ['mean', 'std'],
        'response_time': ['mean', 'std'],
        'efficiency_ratio': 'mean',
        'question_id': 'count'
    }).round(3)

    # é‡å‘½ååˆ—
    overall_stats.columns = ['åŸå§‹æ•ˆç‡å‡å€¼', 'åŸå§‹æ•ˆç‡æ ‡å‡†å·®',
                             'æ ‡å‡†åŒ–æ•ˆç‡å‡å€¼', 'æ ‡å‡†åŒ–æ•ˆç‡æ ‡å‡†å·®',
                             'å“åº”æ—¶é—´å‡å€¼', 'å“åº”æ—¶é—´æ ‡å‡†å·®',
                             'æ•ˆç‡æ¯”å€¼å‡å€¼', 'é¢˜ç›®æ•°é‡']

    print("\nğŸ“ˆ æ€»ä½“æ•ˆç‡æ’å:")
    print(overall_stats.sort_values('åŸå§‹æ•ˆç‡å‡å€¼', ascending=False))

    # æ•ˆç‡ç‹è€…
    raw_winner = df.groupby('model')['raw_efficiency'].mean().idxmax()
    norm_winner = df.groupby('model')['normalized_efficiency'].mean().idxmax()
    fastest = df.groupby('model')['response_time'].mean().idxmin()

    print(f"\nğŸ† æ•ˆç‡ç‹è€…:")
    print(f"  åŸå§‹æ•ˆç‡æœ€é«˜: {raw_winner}")
    print(f"  æ ‡å‡†åŒ–æ•ˆç‡æœ€é«˜: {norm_winner}")
    print(f"  å“åº”æœ€å¿«: {fastest}")

    # æŒ‰éš¾åº¦åˆ†æ
    print(f"\nğŸ¯ æŒ‰éš¾åº¦åˆ†æ:")
    for difficulty in ['Easy', 'Medium', 'Hard']:
        df_diff = df[df['difficulty'] == difficulty]
        if len(df_diff) > 0:
            diff_winner = df_diff.groupby('model')['raw_efficiency'].mean().idxmax()
            avg_time = df_diff['response_time'].mean()
            print(f"  {difficulty}: æœ€ä½³æ¨¡å‹={diff_winner}, å¹³å‡å“åº”æ—¶é—´={avg_time:.2f}ç§’")

    # ç¨³å®šæ€§åˆ†æ
    print(f"\nğŸ“Š ç¨³å®šæ€§åˆ†æ (å“åº”æ—¶é—´æ ‡å‡†å·®):")
    stability = df.groupby('model')['response_time'].std().sort_values()
    for model, std in stability.items():
        print(f"  {model}: {std:.2f}ç§’")

    # æç¤ºè¯å·¥ç¨‹æ•ˆæœåˆ†æ
    print(f"\nğŸ”§ æç¤ºè¯å·¥ç¨‹æ•ˆæœåˆ†æ:")
    prompt_strategies = df['prompt_strategy'].unique()

    for strategy in prompt_strategies:
        df_strategy = df[df['prompt_strategy'] == strategy]
        if len(df_strategy) > 0:
            strategy_winner = df_strategy.groupby('model')['raw_efficiency'].mean().idxmax()
            avg_efficiency = df_strategy['raw_efficiency'].mean()
            avg_time_strategy = df_strategy['response_time'].mean()
            print(
                f"  {strategy}: æœ€ä½³æ¨¡å‹={strategy_winner}, å¹³å‡æ•ˆç‡={avg_efficiency:.2f}, å¹³å‡å“åº”æ—¶é—´={avg_time_strategy:.2f}ç§’")

    # æç¤ºè¯å·¥ç¨‹ç­–ç•¥å¯¹æ¯”
    print(f"\nğŸ“ˆ æç¤ºè¯å·¥ç¨‹ç­–ç•¥å¯¹æ¯”:")
    strategy_comparison = df.groupby('prompt_strategy').agg({
        'raw_efficiency': 'mean',
        'response_time': 'mean',
        'question_id': 'count'
    }).round(3)

    strategy_comparison.columns = ['å¹³å‡æ•ˆç‡', 'å¹³å‡å“åº”æ—¶é—´', 'é¢˜ç›®æ•°é‡']
    print(strategy_comparison.sort_values('å¹³å‡æ•ˆç‡', ascending=False))

    # æœ€ä½³æç¤ºè¯å·¥ç¨‹ç­–ç•¥æ¨è
    print(f"\nğŸ’¡ æœ€ä½³æç¤ºè¯å·¥ç¨‹ç­–ç•¥æ¨è:")
    best_strategy_overall = strategy_comparison.loc[strategy_comparison['å¹³å‡æ•ˆç‡'].idxmax()]
    print(f"  æ€»ä½“æœ€ä½³ç­–ç•¥: {strategy_comparison['å¹³å‡æ•ˆç‡'].idxmax()}")
    print(f"  å¹³å‡æ•ˆç‡: {best_strategy_overall['å¹³å‡æ•ˆç‡']:.2f}")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {best_strategy_overall['å¹³å‡å“åº”æ—¶é—´']:.2f}ç§’")

    # æŒ‰æ¨¡å‹åˆ†ææœ€ä½³ç­–ç•¥
    print(f"\nğŸ¤– å„æ¨¡å‹æœ€ä½³æç¤ºè¯å·¥ç¨‹ç­–ç•¥:")
    model_strategy_analysis = df.groupby(['model', 'prompt_strategy']).agg({
        'raw_efficiency': 'mean',
        'response_time': 'mean'
    }).reset_index()

    for model in df['model'].unique():
        model_data = model_strategy_analysis[model_strategy_analysis['model'] == model]
        if len(model_data) > 0:
            best_for_model = model_data.loc[model_data['raw_efficiency'].idxmax()]
            print(
                f"  {model}: æœ€ä½³ç­–ç•¥={best_for_model['prompt_strategy']}, æ•ˆç‡={best_for_model['raw_efficiency']:.2f}")


def save_analysis_results(df, chart, output_dir="results"):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜å›¾è¡¨
    chart.savefig(os.path.join(output_dir, 'comprehensive_efficiency_analysis.png'),
                  dpi=300, bbox_inches='tight')

    # ä¿å­˜æ•°æ®
    df.to_csv(os.path.join(output_dir, 'efficiency_analysis_data.csv'),
              index=False, encoding='utf-8')

    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
    with open(os.path.join(output_dir, 'analysis_summary.txt'), 'w', encoding='utf-8') as f:
        f.write("æ•ˆç‡åˆ†ææ‘˜è¦æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n")

        # æ€»ä½“ç»Ÿè®¡
        overall_stats = df.groupby('model').agg({
            'raw_efficiency': 'mean',
            'response_time': 'mean',
            'question_id': 'count'
        }).round(3)

        f.write("\næ€»ä½“æ•ˆç‡æ’å:\n")
        f.write(overall_stats.sort_values('raw_efficiency', ascending=False).to_string())

        # æç¤ºè¯å·¥ç¨‹æ•ˆæœ
        prompt_stats = df.groupby('prompt_strategy').agg({
            'raw_efficiency': 'mean',
            'response_time': 'mean'
        }).round(3)

        f.write("\n\næç¤ºè¯å·¥ç¨‹æ•ˆæœ:\n")
        f.write(prompt_stats.to_string())

    print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")


# ä¸»ç¨‹åº
if __name__ == "__main__":
    # è®¾ç½®ä½ çš„æ•°æ®æ ¹ç›®å½•è·¯å¾„
    BASE_PATH = "E:/Preprocessing/AI4S_2/Math/"  # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹

    try:
        # 1. åŠ è½½æ‰€æœ‰JSONæ•°æ®
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        your_json_data = load_all_json_data(BASE_PATH)

        if not your_json_data:
            print("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„è®¾ç½®")
            exit()

        # 2. åˆ†ææ‰€æœ‰é¢˜ç›®
        print("æ­£åœ¨åˆ†ææ•ˆç‡...")
        df_all = comprehensive_efficiency_analysis_all_questions(your_json_data)

        # 3. åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨
        print("æ­£åœ¨ç”Ÿæˆå›¾è¡¨...")
        chart = create_comprehensive_analysis_charts(df_all)

        # 4. æ‰“å°è¯¦ç»†æ´å¯Ÿ
        print_detailed_efficiency_insights(df_all)

        # 5. ä¿å­˜ç»“æœ
        save_analysis_results(df_all, chart)

        # 6. æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        print("\n" + "=" * 80)
        print("æ•°æ®æ ·æœ¬")
        print("=" * 80)
        print(df_all.head(10))

        # 7. æ˜¾ç¤ºå›¾è¡¨
        plt.show()

        print("\nâœ… åˆ†æå®Œæˆï¼")

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()